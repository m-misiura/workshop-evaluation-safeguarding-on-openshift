version: '2'
image_name: vllm-demo-one
apis:
  - inference
  - telemetry
providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL}
        max_tokens: ${env.MAX_TOKENS}
        api_token: ${env.VLLM_API_TOKEN}
        model_name: ${env.INFERENCE_MODEL}
  telemetry:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      service_name: ${env.OTEL_SERVICE_NAME:}
      sinks: ${env.TELEMETRY_SINKS:console,sqlite}
      sqlite_db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/trace_store.db
models:
  - model_id: "meta-llama/Llama-3.1-8B-Instruct"
    provider_id: vllm-inference
    name: "Llama 3.1 8B Instruct"
    description: "Llama language model for text generation"

server:
  port: 5001
  tls_certfile: null
  tls_keyfile: null
  telemetry: true